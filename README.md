Synthetic dataset for training OCR models on Korean comics.

Samples:

-   Detection samples: [1](https://github.com/LiteralGenie/comic_ocr/blob/master/assets/readme_images/detection_1.png) [2](https://github.com/LiteralGenie/comic_ocr/blob/master/assets/readme_images/detection_2.png) [3](https://github.com/LiteralGenie/comic_ocr/blob/master/assets/readme_images/detection_3.png)
-   Recognition samples: [1](https://github.com/LiteralGenie/comic_ocr/blob/master/assets/readme_images/reco_1.png) [2](https://github.com/LiteralGenie/comic_ocr/blob/master/assets/readme_images/reco_2.png) [3](https://github.com/LiteralGenie/comic_ocr/blob/master/assets/readme_images/reco_3.png)
-   Post-training predictions: https://github.com/LiteralGenie/comic_ocr/tree/master/assets/readme_images/sample_outputs 

Pre-trained weights can be found in the [releases section](https://github.com/LiteralGenie/comic_ocr/releases).

This was mainly designed to provide OCR functionality for https://github.com/LiteralGenie/reader

# Setup

Get the code

```bash
git clone git@github.com:LiteralGenie/comic_ocr.git
cd comic_ocr
```

Install dependencies

```bash
python3 -m venv venv
. ./venv/bin/activate
pip install -r requirements.txt
```

Modify the `training section` of the config file as necessary (or leave defaults)

```bash
nano config.toml
```

Generate a vocabulary database

```bash
python comic_ocr/generate_vocab_file.py
```

Add font files as necessary to the `font_dir` folder specified in the config.

Similarly, add images (.png) to the `image_dir` folder.

## Training the detection model

The job of the detection model is to draw bounding boxes around individual words.

Training data for the detection model is generated by rendering text using a random selection of fonts, background colors, background images, rotations, etc.

Generate the dataset by running

```bash
python comic_ocr/generate_detection_dataset.py config.toml

# Use --help to see additional options (like # of samples to generate)
python comic_ocr/generate_detection_dataset.py --help
```

Train the model on the newly-generated dataset

```bash
python comic_ocr/train_detection.py config.toml

# Use --help to see additional options (like learning rate)
python comic_ocr/train_detection.py --help
```

Weights will be saved to the `det_model_dir` folder specified in config.

Model metrics (like accuracy, but not individual outputs) can be inspected by running

```bash
python comic_ocr/evaluate.py config.toml detection data/models/YOUR_WEIGHTS.pt
```

## Training the recognition model

The job of the recognition model is take crops generated by the detection model and output the text it contains.

Training data for the recognition model are crops of the detection data.

Generate the dataset by running

```bash
python comic_ocr/generate_recognition_dataset.py config.toml

# Use --help to see additional options (like # of samples to generate)
python comic_ocr/generate_recognition_dataset.py --help
```

Train the model on the newly-generated dataset

```bash
python comic_ocr/train_recognition.py config.toml

# Use --help to see additional options (like learning rate)
python comic_ocr/train_recognition.py --help
```

Weights will be saved to the `reco_model_dir` folder specified in config.

Model metrics (like accuracy, but not individual outputs) can be inspected by running

```bash
python comic_ocr/evaluate.py config.toml reco data/models/YOUR_WEIGHTS.pt
```

## Visualizing model outputs

```bash
python comic_ocr/inspect_model.py config2.toml path/to/db_resnet50_blah_blah.pt ./parseq_blah_blah.pt path/to/folder/with/images
```

Sample outputs generated by this command can be found [here](https://github.com/LiteralGenie/comic_ocr/tree/master/assets/readme_images/sample_outputs).

# Notes

-   Punctuation is intentionally excluded from the dataset, this will probably be fixed in the future.
